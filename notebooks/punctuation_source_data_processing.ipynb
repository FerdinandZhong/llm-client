{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c032702-7829-48d5-af9f-971d5a6d7827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import zip_longest\n",
    "\n",
    "from plane import CJK\n",
    "\n",
    "from dbpunctuator.data_process import clean_up_data_from_txt, generate_corpus\n",
    "from dbpunctuator.utils import DEFAULT_CHINESE_NER_MAPPING, remove_brackets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be3861fb-150d-4263-a07e-39a6f7934f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# self defined special cleaning func\n",
    "# as the ch training data used is having en puncs\n",
    "def normalize_puncs(input):\n",
    "    normalization = {\"?\": \"？\", \"!\": \"！\", \"（\": \"(\", \"）\": \")\", \"...\": \"。\", \",\": \"，\"}\n",
    "    normalizer = re.compile(\n",
    "        \"({})\".format(\"|\".join(map(re.escape, normalization.keys())))\n",
    "    )\n",
    "    return normalizer.sub(lambda m: normalization[m.string[m.start() : m.end()]], input)\n",
    "\n",
    "\n",
    "def remove_title(input):\n",
    "    \"\"\"remove title inside training data. (title doesn't have period at the end)\"\"\"\n",
    "    if input.strip() and input.strip()[-1] not in [\"。\", \"？\", \"！\"]:\n",
    "        return \"\"\n",
    "    return input\n",
    "\n",
    "\n",
    "def revert_ascii_chars_whitespace(input):\n",
    "    \"\"\"revert the original data to remove spaces between latin chars\n",
    "\n",
    "    Args:\n",
    "        input (string): input to be processed\n",
    "\n",
    "    \"\"\"\n",
    "    regex = re.compile(\"(?P<%s>%s)\" % (CJK.name, CJK.pattern), CJK.flag)\n",
    "    result = \"\"\n",
    "    start = 0\n",
    "    for t in regex.finditer(input):\n",
    "        result += \" \" + \"\".join(\n",
    "            [char for char in list(input[start : t.start()]) if char != \" \"]\n",
    "        )\n",
    "        result += \" \" + input[t.start() : t.end()]\n",
    "        start = t.end()\n",
    "    result += input[start:]\n",
    "    return result\n",
    "\n",
    "\n",
    "def merge_data(whole_data_path, *tokens_data_paths):\n",
    "    all_lines = []\n",
    "    with open(whole_data_path, \"w+\") as whole_data_file:\n",
    "        for cleaned_data_path in tokens_data_paths:\n",
    "            with open(cleaned_data_path, \"r\") as data_file:\n",
    "                all_lines.append(data_file.readlines())\n",
    "        for lines in zip_longest(*all_lines):\n",
    "            for line in lines:\n",
    "                if line:\n",
    "                    whole_data_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c833d4a-15ab-4da2-9fdb-0f482b0f5036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_whitespace_zh_chars(input):\n",
    "    \"\"\"revert the original data to remove spaces between latin chars\n",
    "\n",
    "    Args:\n",
    "        input (string): input to be processed\n",
    "\n",
    "    \"\"\"\n",
    "    regex = re.compile(\"(?P<%s>%s)\" % (CJK.name, CJK.pattern), CJK.flag)\n",
    "    result = \"\"\n",
    "    start = 0\n",
    "    for t in regex.finditer(input):\n",
    "        result += input[start : t.start()]\n",
    "        result += \" \" + \" \".join(\n",
    "            [char for char in list(input[t.start() : t.end()]) if char != \" \"]\n",
    "        ) + \" \"\n",
    "        start = t.end()\n",
    "    result += input[start:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c184a0b2-5dbd-4937-b026-45c53115104d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 第 二 环 节 ，  角 色 扮 演 。  你 好 ，  我 刚 到 新 加 坡 ，  我 是 马 来 西 亚 的 交 换 的 学 生 ，  我 来 南 大 交 流 学 习 。  对 新 加 坡 还 不 是 很 熟 悉 ，  请 问 ， ah  你 可 以 推 荐 我 几 个 景 点 吗 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"第二环节， 角色扮演。 你好， 我刚到新加坡， 我是马来西亚的交换的学生， 我来南大交流学习。 对新加坡还不是很熟悉， 请问， ah 你可以推荐我几个景点吗。\\n\"\n",
    "\n",
    "print(add_whitespace_zh_chars(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1c11b8e-dcd6-42c4-a23e-59b40fd85d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/cleaned_test.txt\", \"r\") as file:\n",
    "    source_data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e7c29d5-3fb6-48e8-a488-f7d1b6d5f9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['第二环节， 角色扮演。 你好， 我刚到新加坡， 我是马来西亚的交换的学生， 我来南大交流学习。 对新加坡还不是很熟悉， 请问， ah 你可以推荐我几个景点吗。\\n',\n",
       " '我有听过 eh， 是不是很像那个 london eye 一样。\\n',\n",
       " '哦。\\n',\n",
       " '那你是比较推荐我白天去还是晚上去呢。\\n',\n",
       " 'orh 就是夜景比较美啦。 哦那我会看到哪里就是什么样的景象呢。 但是因为你知道新加坡都是高楼大厦嘛， 对不对。 那如果我在上面的我是会会鸟览整个新加坡吗。 还是，\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0539445b-b889-4478-a571-e7f94693889b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:23:43,306 - \u001b[32mINFO\u001b[0m - data_cleanning.py:73 - data_cleanning.text_lines_cleaning - 947 - clean up text file line by line.\n",
      "2023-10-15 23:23:43,309 - \u001b[32mINFO\u001b[0m - data_cleanning.py:74 - data_cleanning.text_lines_cleaning - 947 - replace email with <EMAIL>\n",
      "2023-10-15 23:23:43,311 - \u001b[32mINFO\u001b[0m - data_cleanning.py:75 - data_cleanning.text_lines_cleaning - 947 - replace url with <URL>\n",
      "2023-10-15 23:23:43,312 - \u001b[32mINFO\u001b[0m - data_cleanning.py:76 - data_cleanning.text_lines_cleaning - 947 - replace currency with <CURRENCY>\n",
      "2023-10-15 23:23:43,313 - \u001b[32mINFO\u001b[0m - data_cleanning.py:77 - data_cleanning.text_lines_cleaning - 947 - replace telephone with <TEL>\n",
      "2023-10-15 23:23:43,314 - \u001b[32mINFO\u001b[0m - data_cleanning.py:78 - data_cleanning.text_lines_cleaning - 947 - replace number with <NUM>\n",
      "100%|██████████| 11576/11576 [00:01<00:00, 7477.11it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_from_txt(\n",
    "    source_data,\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/cleaned_test.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    "    special_cleaning_funcs=[\n",
    "        remove_brackets_text,\n",
    "        add_whitespace_zh_chars,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20f982c8-86d0-4f28-affb-561e919c7e77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:24:01,057 - \u001b[32mINFO\u001b[0m - data_process.py:172 - data_process.generate_corpus - 947 - generate training data\n",
      "100%|██████████| 11576/11576 [00:00<00:00, 82262.02it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_corpus(\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/cleaned_test.txt\",\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/test_token_tag_data.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de26635-c3ef-46c9-90b3-b66e0c667220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:31:29,211 - \u001b[32mINFO\u001b[0m - data_cleanning.py:73 - data_cleanning.text_lines_cleaning - 947 - clean up text file line by line.\n",
      "2023-10-15 23:31:29,212 - \u001b[32mINFO\u001b[0m - data_cleanning.py:74 - data_cleanning.text_lines_cleaning - 947 - replace email with <EMAIL>\n",
      "2023-10-15 23:31:29,213 - \u001b[32mINFO\u001b[0m - data_cleanning.py:75 - data_cleanning.text_lines_cleaning - 947 - replace url with <URL>\n",
      "2023-10-15 23:31:29,214 - \u001b[32mINFO\u001b[0m - data_cleanning.py:76 - data_cleanning.text_lines_cleaning - 947 - replace currency with <CURRENCY>\n",
      "2023-10-15 23:31:29,216 - \u001b[32mINFO\u001b[0m - data_cleanning.py:77 - data_cleanning.text_lines_cleaning - 947 - replace telephone with <TEL>\n",
      "2023-10-15 23:31:29,217 - \u001b[32mINFO\u001b[0m - data_cleanning.py:78 - data_cleanning.text_lines_cleaning - 947 - replace number with <NUM>\n",
      "100%|██████████| 11938/11938 [00:01<00:00, 7465.89it/s]\n",
      "2023-10-15 23:31:34,879 - \u001b[32mINFO\u001b[0m - data_process.py:172 - data_process.generate_corpus - 947 - generate training data\n",
      "100%|██████████| 11938/11938 [00:00<00:00, 72640.28it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/cleaned_dev.txt\", \"r\") as file:\n",
    "    source_data = file.readlines()\n",
    "    \n",
    "clean_up_data_from_txt(\n",
    "    source_data,\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/cleaned_dev.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    "    special_cleaning_funcs=[\n",
    "        remove_brackets_text,\n",
    "        add_whitespace_zh_chars,\n",
    "    ],\n",
    ")\n",
    "\n",
    "generate_corpus(\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/cleaned_dev.txt\",\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/token_tag_files/dev_token_tag_data.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f977cfe-162f-4af2-aee4-614b3e2b8969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:31:58,222 - \u001b[32mINFO\u001b[0m - data_cleanning.py:73 - data_cleanning.text_lines_cleaning - 947 - clean up text file line by line.\n",
      "2023-10-15 23:31:58,224 - \u001b[32mINFO\u001b[0m - data_cleanning.py:74 - data_cleanning.text_lines_cleaning - 947 - replace email with <EMAIL>\n",
      "2023-10-15 23:31:58,225 - \u001b[32mINFO\u001b[0m - data_cleanning.py:75 - data_cleanning.text_lines_cleaning - 947 - replace url with <URL>\n",
      "2023-10-15 23:31:58,226 - \u001b[32mINFO\u001b[0m - data_cleanning.py:76 - data_cleanning.text_lines_cleaning - 947 - replace currency with <CURRENCY>\n",
      "2023-10-15 23:31:58,227 - \u001b[32mINFO\u001b[0m - data_cleanning.py:77 - data_cleanning.text_lines_cleaning - 947 - replace telephone with <TEL>\n",
      "2023-10-15 23:31:58,228 - \u001b[32mINFO\u001b[0m - data_cleanning.py:78 - data_cleanning.text_lines_cleaning - 947 - replace number with <NUM>\n",
      "100%|██████████| 96176/96176 [00:12<00:00, 7614.09it/s]\n",
      "2023-10-15 23:32:43,726 - \u001b[32mINFO\u001b[0m - data_process.py:172 - data_process.generate_corpus - 947 - generate training data\n",
      "100%|██████████| 96176/96176 [00:01<00:00, 82205.05it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/cleaned_train.txt\", \"r\") as file:\n",
    "    source_data = file.readlines()\n",
    "    \n",
    "clean_up_data_from_txt(\n",
    "    source_data,\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/cleaned_train.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    "    special_cleaning_funcs=[\n",
    "        remove_brackets_text,\n",
    "        add_whitespace_zh_chars,\n",
    "    ],\n",
    ")\n",
    "\n",
    "generate_corpus(\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/processed/cleaned_train.txt\",\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/token_tag_files/train_token_tag_data.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae3ffe-c189-4fb8-9247-ba046479303b",
   "metadata": {},
   "source": [
    "## Generate Token Tag File for LLM Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd1b32-eb0f-4686-8fbb-008b8e169b5a",
   "metadata": {},
   "source": [
    "### LLAMA2 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4437d576-5b20-4b1a-8a91-e23f5a7f9239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/llm_results/llama2_13b/train_results.txt\", \"r\") as file:\n",
    "    source_data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9ac5aa-a897-4038-8152-17f0819fb8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['好，第三环节，扮演，好，你好，我刚来到新加坡，我是来自马来西亚的交换生啊，所以我对新加坡就不是很熟悉啦，所以我们目前是在NTU，我想问一下要怎么如何从NTU去榜鹅呢，因为榜鹅有那个海鲜嘛，对对对，对对对，那可以跟我讲是在啊，对对对，一个星期后，所以在新加坡待一段时间。\\n',\n",
       " '啦，oh 没有啦，因为一个星期嘛就要问你很多问题，uh 是住在 hotel ，也是在市中心啊，bugis，but 你可以跟我讲一下那个，er 海鲜吃海鲜地方在哪里吗，okay 哦，可以跟我讲一下那边也有怎么样的美食呢，uh 你就由你来介绍吧，okay，oh，jumbo 啊嗯，okay，那除了还有不是它还有好几间分店吗哦，okay，okay，嗯嗯嗯，okay，那我想问一下那还有其他的分\\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c9e8b9-efb8-476c-9e0e-7514cdceccc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 23:14:10,423 - \u001b[32mINFO\u001b[0m - data_cleanning.py:73 - data_cleanning.text_lines_cleaning - 1720 - clean up text file line by line.\n",
      "2023-10-25 23:14:10,425 - \u001b[32mINFO\u001b[0m - data_cleanning.py:74 - data_cleanning.text_lines_cleaning - 1720 - replace email with <EMAIL>\n",
      "2023-10-25 23:14:10,426 - \u001b[32mINFO\u001b[0m - data_cleanning.py:75 - data_cleanning.text_lines_cleaning - 1720 - replace url with <URL>\n",
      "2023-10-25 23:14:10,428 - \u001b[32mINFO\u001b[0m - data_cleanning.py:76 - data_cleanning.text_lines_cleaning - 1720 - replace currency with <CURRENCY>\n",
      "2023-10-25 23:14:10,430 - \u001b[32mINFO\u001b[0m - data_cleanning.py:77 - data_cleanning.text_lines_cleaning - 1720 - replace telephone with <TEL>\n",
      "2023-10-25 23:14:10,430 - \u001b[32mINFO\u001b[0m - data_cleanning.py:78 - data_cleanning.text_lines_cleaning - 1720 - replace number with <NUM>\n",
      "100%|██████████| 10109/10109 [00:03<00:00, 3361.31it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_from_txt(\n",
    "    source_data,\n",
    "    \"/root/autodl-tmp/datasets/mml-zh/llm_results/llama2_13b/train_results_cleaned.txt\",\n",
    "    ner_mapping=DEFAULT_CHINESE_NER_MAPPING,\n",
    "    special_cleaning_funcs=[\n",
    "        normalize_puncs,\n",
    "        add_whitespace_zh_chars,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13e11b14-e7e8-48f5-b562-0de4b3e4aed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(\"[a-zA-Z]{2,}\")\n",
    "print(bool(re.match(regex, \"nt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d71f7f3-eafd-4329-9e6f-789b4299db76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d43825-2225-4f1d-a354-046821d46812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-tgi-env",
   "language": "python",
   "name": "llm-tgi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

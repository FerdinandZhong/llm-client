{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e22515-e8e5-4dbd-9679-9057c9d55667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c5d31d-b5bc-4e38-9a8d-6abbf2e61488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _read_data(\n",
    "    source_data, target_sequence_length, is_return_list\n",
    ") -> Union[List[List], List[str]]:\n",
    "    def read_line(text_line):\n",
    "        return text_line.strip().split(\"\\t\")\n",
    "\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    line_index = 0\n",
    "\n",
    "    token_doc = []\n",
    "    tag_doc = []\n",
    "    if isinstance(source_data, List):\n",
    "        pbar = tqdm(source_data)\n",
    "    else:\n",
    "        with open(source_data, \"r\") as data_file:\n",
    "            pbar = tqdm(data_file.readlines())\n",
    "    last_is_en_word = False\n",
    "    for index, line in enumerate(pbar):\n",
    "        if line == \"\\n\":\n",
    "            continue\n",
    "        processed_line = read_line(line)\n",
    "        try:\n",
    "            assert len(processed_line) == 2, \"bad line\"\n",
    "            token, tag = processed_line\n",
    "            en_regex = re.compile(\"[a-zA-Z+]\")\n",
    "            if not bool(re.match(en_regex, token)):\n",
    "                if last_is_en_word:\n",
    "                    token_doc.append(f\" {token}\")\n",
    "                else:\n",
    "                    token_doc.append(token)\n",
    "                last_is_en_word = False\n",
    "            else:\n",
    "                last_is_en_word = True\n",
    "                token_doc.append(f\" {token}\")\n",
    "            tag_doc.append(processed_line[1])\n",
    "        except AssertionError:\n",
    "            print(f\"ignore the bad line: {line}, index: {index}\")\n",
    "            continue\n",
    "        line_index += 1\n",
    "        if len(token_doc) >= target_sequence_length:\n",
    "            try:\n",
    "                _verify_senquence(token_doc, target_sequence_length)\n",
    "                _verify_senquence(tag_doc, target_sequence_length)\n",
    "                if is_return_list:\n",
    "                    token_docs.append(token_doc)\n",
    "                else:\n",
    "                    token_docs.append(\"\".join(token_doc))\n",
    "                tag_docs.append(tag_doc)\n",
    "                token_doc = []\n",
    "                tag_doc = []\n",
    "            except AssertionError:\n",
    "                print(f\"error generating sequence: {token_doc}\")\n",
    "                token_doc = []\n",
    "                tag_doc = []\n",
    "                continue\n",
    "            pbar.update(len(token_doc))\n",
    "    try:\n",
    "        assert (len(token_doc)==len(tag_doc)), \"Not equal length\"\n",
    "        if is_return_list:\n",
    "            token_docs.append(token_doc)\n",
    "        else:\n",
    "            token_docs.append(\"\".join(token_doc))\n",
    "        tag_docs.append(tag_doc)\n",
    "        pbar.update(len(token_doc))\n",
    "    except AssertionError:\n",
    "        print(f\"error generating sequence: {token_doc}\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "\n",
    "def _verify_senquence(sequence, target_sequence_length):\n",
    "    assert (\n",
    "        target_sequence_length <= len(sequence)\n",
    "    ), \"wrong sequence length\"\n",
    "\n",
    "\n",
    "def process_data(\n",
    "    source_data, target_sequence_length, is_return_list=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for generation of tokenized corpus and relevant tags\n",
    "\n",
    "    Args:\n",
    "        source_data(str or List): path of input data or input data\n",
    "        target_sequence_length(int): target sequence length of one sample\n",
    "    \"\"\"\n",
    "    print(\"load data\")\n",
    "    texts, tags = _read_data(\n",
    "        source_data,\n",
    "        target_sequence_length,\n",
    "        is_return_list=is_return_list,\n",
    "    )\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19115fda-1b5e-4bb2-b056-b3493ff59df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1220989/1220989 [00:04<00:00, 279991.02it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/token_tag_files/train_token_tag_data.txt\", \"r\") as fb:\n",
    "    train_source_data = fb.readlines()\n",
    "\n",
    "train_texts, train_tags = process_data(train_source_data, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0700b625-38a0-4a14-ac3e-2ec789a2b0ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8788"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eabc667f-b954-4fed-b2d6-39a259531b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' okay', ' 好', '第', '三', '环', '节', ' er', ' 角', '色', '扮', '演', ' okay', ' hello', ' 你', '好', ' oh', ' 我', '刚', '来', '到', '新', '加', '坡', '我', '是', '来', '自', ' malaysia', ' 的', '交', '换', '生', '啊', '所', '以', '我', '对', '新', '加', '坡', '就', '不', '是', '很', '熟', '悉', '啦', ' so', ' 我', '们', '目', '前', '是', '在', ' er', ' n', ' t', ' u', ' so', ' 我', '想', '问', '一', '下', '要', '怎', '么', '如', '何', '从', ' n', ' t', ' u', ' 去', ' er', ' 榜', '鹅', '呢', '对', '因', '为', '榜', '鹅', '有', '那', '个', ' uh', ' 海', '鲜', '嘛', '对', '对', '对', ' oh', ' okay', ' 那', '可', '以', '跟', '我', '讲', '是', '在', '啊', '对', '对', '对', '几', '时', '回', '来', '啊', '一', '个', '星', '期', '后', '所', '以', '在', '新', '加', '坡', '待', '一', '段', '时', '间'] ['C_COMMA', 'C_COMMA', 'O', 'O', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'C_PERIOD', 'C_COMMA', 'O', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_PERIOD', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_PERIOD', 'C_PERIOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_PERIOD', 'O', 'O', 'C_PERIOD', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'C_PERIOD', 'C_PERIOD', 'O', 'O', 'C_PERIOD', 'O', 'O', 'O', 'O', 'C_PERIOD', 'O', 'O', 'O', 'O', 'C_COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0], train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd1d6d35-a264-4c8f-af16-2e14dbc362b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/llm_results/ground_truth_texts.txt\", \"w\") as output_file:\n",
    "    for text in train_texts:\n",
    "        output_file.write(\"\".join(text)+\"\\n\")\n",
    "\n",
    "with open(\"/root/autodl-tmp/datasets/mml-zh/llm_results/ground_truth_tags.txt\", \"w\") as output_file:\n",
    "    for tag in train_tags:\n",
    "        output_file.write(\" \".join(tag)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67ca64-9572-4916-8ddc-0371e3e30fdb",
   "metadata": {},
   "source": [
    "## Prediction with llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59103285-2454-42e9-975f-de97629743b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 23:47:54,823 - \u001b[32mINFO\u001b[0m - pipeline.py:20 - pipeline.__init__ - 2064 - parameters for every request: {'do_sample': False, 'max_new_tokens': 256, 'repetition_penalty': None, 'return_full_text': False, 'seed': None, 'temperature': None, 'top_k': None, 'top_p': None, 'truncate': None, 'typical_p': None, 'best_of': None, 'watermark': False, 'decoder_input_details': False, 'stop_sequences': ['</s>', '[/INST]', '[/SYS>>', 'Question']}\n"
     ]
    }
   ],
   "source": [
    "from llm_client.pipeline import Pipeline\n",
    "from typing import List\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "config_yaml = \"/root/llm_client/config_yamls/llama2-hf.yaml\"\n",
    "\n",
    "pipeline = Pipeline(config_yaml, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1af08fb-b9ba-463c-a254-e408723fcb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_list = []\n",
    "\n",
    "example=\"\"\"\\n[INST] Restore punctuations to the following sentence: \"您好请问您能告诉我去sengkangmrtstation要怎么走吗\"[/INST]\n",
    "\\nAnswer: 您好,请问您能告诉我去 sengkang mrt station 要怎么走吗?\\n\n",
    "\"\"\"\n",
    "\n",
    "for train_text_list in train_texts:\n",
    "    pure_text = \"\".join(train_text_list)\n",
    "    input_list.append(f\"{example}\\n[INST] Restore punctuations to the following sentence: \\\"{pure_text}\\\"[/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e461408-5e30-44d2-b737-dba874e8aebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INST] Restore punctuations to the following sentence: \"您好请问您能告诉我去sengkangmrtstation要怎么走吗\"[/INST]\n",
      "\n",
      "Answer: 您好,请问您能告诉我去 sengkang mrt station 要怎么走吗?\n",
      "\n",
      "\n",
      "[INST] Restore punctuations to the following sentence: \" okay 好第三环节 er 角色扮演 okay hello 你好 oh 我刚来到新加坡我是来自 malaysia 的交换生啊所以我对新加坡就不是很熟悉啦 so 我们目前是在 er n t u so 我想问一下要怎么如何从 n t u 去 er 榜鹅呢对因为榜鹅有那个 uh 海鲜嘛对对对 oh okay 那可以跟我讲是在啊对对对几时回来啊一个星期后所以在新加坡待一段时间\"[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(input_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4bddbf5-3c0e-4128-b065-94f6d3cc1c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def main(input_list: List, pipeline: Pipeline):\n",
    "    tasks = [pipeline.model_predict(input) for input in input_list]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0264aac-4ae5-4ea8-97d5-def6c753d51b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好，第三环节，er角色扮演，okay，你好，oh，我刚来到新加坡，我是来自马来西亚的交换生啊，所以我对新加坡就不是很熟悉啦，so，我们目前是在NTU，我想问一下要怎么如何从NTU去榜鹅呢，对，因为榜鹅有那个uh，海鲜嘛，对，对，对，oh，okay，那可以跟我讲是在啊，对，对，对，几时回来啊，一个星期后，所以在新加坡待一段时间。', '啦，oh 没有啦，因为一个星期嘛就要问你很多问题，uh 是住在 hotel ，也是在市中心啊，bugis，but 你可以跟我讲一下那个，er 海鲜吃海鲜地方在哪里吗，okay 哦，可以跟我讲一下那边也有怎么样的美食呢，uh 你就由你来介绍吧，okay，oh，jumbo 啊嗯，okay，那除了还有不是它还有好几间分店吗哦，okay，okay，嗯嗯嗯，okay，那我想问一下那还有其他的分']\n"
     ]
    }
   ],
   "source": [
    "sample_list = input_list[:2]\n",
    "\n",
    "result_list = await main(sample_list, pipeline)\n",
    "print([result_text.split(\"\\n\\n[INST] Restore punctuations\")[0] for result_text in result_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4c60b-d2d7-4024-9e68-3532ec20acda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8788 [02:44<?, ?it/s]\n",
      "  9%|▉         | 800/8788 [08:43<1:27:19,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "train_result_list = []\n",
    "chunk_size = 50\n",
    "\n",
    "question_list = input_list.copy()\n",
    "pbar = tqdm(total = len(question_list))\n",
    "while len(question_list) > chunk_size:\n",
    "    current_chunk = question_list[:chunk_size]\n",
    "    question_list = question_list[chunk_size:]\n",
    "    \n",
    "    result_list = await main(current_chunk, pipeline)\n",
    "    train_result_list.extend([result_text.split(\"\\n\\n[INST] Restore punctuations\")[0] for result_text in result_list])\n",
    "    \n",
    "    pbar.update(chunk_size)\n",
    "    time.sleep(2)\n",
    "\n",
    "result_list = await main(question_list, pipeline)\n",
    "train_result_list.extend([result_text.split(\"\\n\\n[INST] Restore punctuations\")[0] for result_text in result_list])\n",
    "pbar.update(len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8dafc-6a65-43e7-a3d0-f6f12db76ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/llm_results/llama2_13b/train_results.txt\", \"w\") as output_file:\n",
    "    for result_text in train_result_list:\n",
    "        output_file.write(f\"{result_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5367ab-946a-4959-a189-ee0e5c75c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-tgi-env",
   "language": "python",
   "name": "llm-tgi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e22515-e8e5-4dbd-9679-9057c9d55667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c5d31d-b5bc-4e38-9a8d-6abbf2e61488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _read_data(\n",
    "    source_data, target_sequence_length, is_return_list\n",
    ") -> Union[List[List], List[str]]:\n",
    "    def read_line(text_line):\n",
    "        return text_line.strip().split(\"\\t\")\n",
    "\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    line_index = 0\n",
    "\n",
    "    token_doc = []\n",
    "    tag_doc = []\n",
    "    if isinstance(source_data, List):\n",
    "        pbar = tqdm(source_data)\n",
    "    else:\n",
    "        with open(source_data, \"r\") as data_file:\n",
    "            pbar = tqdm(data_file.readlines())\n",
    "    for index, line in enumerate(pbar):\n",
    "        if line == \"\\n\":\n",
    "        #     token_docs.append(token_doc)\n",
    "        #     tag_docs.append(tag_doc)\n",
    "        #     pbar.update(len(token_doc))\n",
    "        #     token_doc = []\n",
    "        #     tag_doc = []\n",
    "            continue\n",
    "        processed_line = read_line(line)\n",
    "        try:\n",
    "            assert len(processed_line) == 2, \"bad line\"\n",
    "            token, tag = processed_line\n",
    "            token_doc.append(token)\n",
    "            tag_doc.append(processed_line[1])\n",
    "        except AssertionError:\n",
    "            print(f\"ignore the bad line: {line}, index: {index}\")\n",
    "            continue\n",
    "        line_index += 1\n",
    "        if len(token_doc) >= target_sequence_length:\n",
    "            try:\n",
    "                _verify_senquence(token_doc, target_sequence_length)\n",
    "                _verify_senquence(tag_doc, target_sequence_length)\n",
    "                if is_return_list:\n",
    "                    token_docs.append(token_doc)\n",
    "                else:\n",
    "                    token_docs.append(\"\".join(token_doc))\n",
    "                tag_docs.append(tag_doc)\n",
    "                token_doc = []\n",
    "                tag_doc = []\n",
    "            except AssertionError:\n",
    "                print(f\"error generating sequence: {token_doc}\")\n",
    "                token_doc = []\n",
    "                tag_doc = []\n",
    "                continue\n",
    "            pbar.update(len(token_doc))\n",
    "    try:\n",
    "        assert (len(token_doc)==len(tag_doc)), \"Not equal length\"\n",
    "        if is_return_list:\n",
    "            token_docs.append(token_doc)\n",
    "        else:\n",
    "            token_docs.append(\"\".join(token_doc))\n",
    "        tag_docs.append(tag_doc)\n",
    "        pbar.update(len(token_doc))\n",
    "    except AssertionError:\n",
    "        print(f\"error generating sequence: {token_doc}\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "\n",
    "def _verify_senquence(sequence, target_sequence_length):\n",
    "    assert (\n",
    "        target_sequence_length <= len(sequence)\n",
    "    ), \"wrong sequence length\"\n",
    "\n",
    "\n",
    "def process_data(\n",
    "    source_data, target_sequence_length, is_return_list=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for generation of tokenized corpus and relevant tags\n",
    "\n",
    "    Args:\n",
    "        source_data(str or List): path of input data or input data\n",
    "        target_sequence_length(int): target sequence length of one sample\n",
    "    \"\"\"\n",
    "    print(\"load data\")\n",
    "    texts, tags = _read_data(\n",
    "        source_data,\n",
    "        target_sequence_length,\n",
    "        is_return_list=is_return_list,\n",
    "    )\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19115fda-1b5e-4bb2-b056-b3493ff59df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1220989/1220989 [00:01<00:00, 1054794.90it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/token_tag_files/train_token_tag_data.txt\", \"r\") as fb:\n",
    "    train_source_data = fb.readlines()\n",
    "\n",
    "train_texts, train_tags = process_data(train_source_data, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0700b625-38a0-4a14-ac3e-2ec789a2b0ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabc667f-b954-4fed-b2d6-39a259531b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okay', '好', '第', '三', '环', '节', 'er', '角', '色', '扮', '演', 'okay', 'hello', '你', '好', 'oh', '我', '刚', '来', '到', '新', '加', '坡', '我', '是', '来', '自', 'malaysia', '的', '交', '换', '生', '啊', '所', '以', '我', '对', '新', '加', '坡', '就', '不', '是', '很', '熟', '悉', '啦', 'so', '我', '们', '目', '前', '是', '在', 'er', 'n', 't', 'u', 'so', '我', '想', '问', '一', '下', '要', '怎', '么', '如', '何', '从', 'n', 't', 'u', '去', 'er', '榜', '鹅', '呢', '对', '因', '为', '榜', '鹅', '有', '那', '个', 'uh', '海', '鲜', '嘛', '对', '对', '对', 'oh', 'okay', '那', '可', '以', '跟', '我', '讲', '是', '在', '啊', '对', '对', '对', '几', '时', '回', '来', '啊', '一', '个', '星', '期', '后', '所', '以', '在', '新', '加', '坡', '待', '一', '段', '时', '间', '啦', 'oh', '没', '有', '啦', '因', '为', '一', '个', '星', '期', '嘛', '就', '要', '问', '你', '很', '多', '问', '题', 'uh', '是', '住', '在', 'hotel', 'uh', '也', '是', '在', '市', '中', '心', '啊', 'bugis', 'but', '你', '可', '以', '跟', '我', '讲', '一', '下', '那', '个', 'er', '海', '鲜', '吃', '海', '鲜', '地', '方', '在', '哪', '里', '吗', 'okay', '哦', '可', '以', '跟', '我', '讲', '一', '下', '那', '边', '也', '有', '怎', '么', '样', '的', '美', '食', '呢', 'uh', '你', '就', '由', '你', '来', '介', '绍', '吧', 'okay', 'oh', 'jumbo', '啊', '嗯', 'okay', '那', '除', '了', '还', '有', '不', '是', '它', '还', '有', '好', '几', '间', '分', '店', '吗', '哦', 'okay', 'okay', '嗯', '嗯', '嗯', 'okay', '那', '我', '想', '问', '一', '下', '那', '还', '有', '其', '他', '的', '分']\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67ca64-9572-4916-8ddc-0371e3e30fdb",
   "metadata": {},
   "source": [
    "## Prediction with llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59103285-2454-42e9-975f-de97629743b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm-tgi-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-22 22:38:21,125 - \u001b[32mINFO\u001b[0m - pipeline.py:20 - pipeline.__init__ - 10707 - parameters for every request: {'do_sample': False, 'max_new_tokens': 256, 'repetition_penalty': None, 'return_full_text': False, 'seed': None, 'temperature': None, 'top_k': None, 'top_p': None, 'truncate': None, 'typical_p': None, 'best_of': None, 'watermark': False, 'decoder_input_details': False, 'stop_sequences': ['</s>', '[/INST]', '[/SYS>>', 'Question']}\n"
     ]
    }
   ],
   "source": [
    "from llm_client.pipeline import Pipeline\n",
    "from typing import List\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "config_yaml = \"/root/llm_client/config_yamls/llama2-hf.yaml\"\n",
    "\n",
    "pipeline = Pipeline(config_yaml, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1af08fb-b9ba-463c-a254-e408723fcb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_list = []\n",
    "\n",
    "for train_text_list in train_texts:\n",
    "    pure_text = \"\".join(train_text_list)\n",
    "    input_list.append(f\"Restore punctuations to the following sentence: '{pure_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e461408-5e30-44d2-b737-dba874e8aebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore punctuations to the following sentence: 'okay好第三环节er角色扮演okayhello你好oh我刚来到新加坡我是来自malaysia的交换生啊所以我对新加坡就不是很熟悉啦so我们目前是在erntuso我想问一下要怎么如何从ntu去er榜鹅呢对因为榜鹅有那个uh海鲜嘛对对对ohokay那可以跟我讲是在啊对对对几时回来啊一个星期后所以在新加坡待一段时间啦oh没有啦因为一个星期嘛就要问你很多问题uh是住在hoteluh也是在市中心啊bugisbut你可以跟我讲一下那个er海鲜吃海鲜地方在哪里吗okay哦可以跟我讲一下那边也有怎么样的美食呢uh你就由你来介绍吧okayohjumbo啊嗯okay那除了还有不是它还有好几间分店吗哦okayokay嗯嗯嗯okay那我想问一下那还有其他的分'\n"
     ]
    }
   ],
   "source": [
    "print(input_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bddbf5-3c0e-4128-b065-94f6d3cc1c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def main(input_list: List, pipeline: Pipeline):\n",
    "    tasks = [pipeline.model_predict(input) for input in input_list]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a4c60b-d2d7-4024-9e68-3532ec20acda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm-tgi-env/lib/python3.9/site-packages/text_generation/client.py:385\u001b[0m, in \u001b[0;36mAsyncClient.generate\u001b[0;34m(self, prompt, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ClientSession(\n\u001b[1;32m    383\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, cookies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m    384\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url, json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdict()) \u001b[38;5;28;01mas\u001b[39;00m resp:\n\u001b[1;32m    386\u001b[0m         payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-tgi-env/lib/python3.9/site-packages/aiohttp/client.py:1141\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-tgi-env/lib/python3.9/site-packages/aiohttp/client.py:560\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-tgi-env/lib/python3.9/site-packages/aiohttp/client_reqrep.py:899\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    898\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\n\u001b[0;32m--> 899\u001b[0m     message, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mHttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-tgi-env/lib/python3.9/site-packages/aiohttp/streams.py:616\u001b[0m, in \u001b[0;36mDataQueue.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mCancelledError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     current_chunk \u001b[38;5;241m=\u001b[39m question_list[:chunk_size]\n\u001b[1;32m      7\u001b[0m     question_list \u001b[38;5;241m=\u001b[39m question_list[chunk_size:]\n\u001b[0;32m----> 9\u001b[0m     train_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;01mawait\u001b[39;00m main(current_chunk, pipeline))\n\u001b[1;32m     11\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m train_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;01mawait\u001b[39;00m main(question_list, pipeline))\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_list, pipeline)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(input_list: List, pipeline: Pipeline):\n\u001b[1;32m      2\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [pipeline\u001b[38;5;241m.\u001b[39mmodel_predict(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m input_list]\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result_list = []\n",
    "chunk_size = 50\n",
    "\n",
    "question_list = input_list.copy()\n",
    "pbar = tqdm(total = len(question_list))\n",
    "while len(question_list) > chunk_size:\n",
    "    current_chunk = question_list[:chunk_size]\n",
    "    question_list = question_list[chunk_size:]\n",
    "    \n",
    "    train_result_list.extend(await main(current_chunk, pipeline))\n",
    "    \n",
    "    pbar.update(chunk)\n",
    "    time.sleep(2)\n",
    "\n",
    "train_result_list.extend(await main(question_list, pipeline))\n",
    "pbar.update(len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8dafc-6a65-43e7-a3d0-f6f12db76ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/root/autodl-tmp/datasets/mml-zh/llm_results/llama2_13b/train_results.txt\", \"w\") as output_file:\n",
    "    for result_text in train_result_list:\n",
    "        output_file.write(f\"{result_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5367ab-946a-4959-a189-ee0e5c75c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-tgi-env",
   "language": "python",
   "name": "llm-tgi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
